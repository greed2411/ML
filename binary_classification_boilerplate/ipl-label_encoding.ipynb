{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Game ID</th>\n",
       "      <th>Team 1</th>\n",
       "      <th>Team 2</th>\n",
       "      <th>City</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DateOfGame</th>\n",
       "      <th>TimeOfGame</th>\n",
       "      <th>AvgWindSpeed</th>\n",
       "      <th>AvgHumidity</th>\n",
       "      <th>Inn 1 Team 1 NOP R&gt;25,SR&gt;125</th>\n",
       "      <th>...</th>\n",
       "      <th>Inn 2 Team 2 NOP R&lt;25, SR&gt;125</th>\n",
       "      <th>Inn 2 Team 2 Total 4s</th>\n",
       "      <th>Inn 2 Team 2 Total 6s</th>\n",
       "      <th>Inn 2 Team 2 Max Strike Rate_ALLBatsmen</th>\n",
       "      <th>Inn 2 Team 1 NoP fast bowlers</th>\n",
       "      <th>Inn 2 Team 1 NoP Spinners</th>\n",
       "      <th>Inn 2 Team 1 wickets taken_catches_runout</th>\n",
       "      <th>Inn2 Team 1 wickets taken_ bowled _lbw_caught by keeper_stumping</th>\n",
       "      <th>Inn 2 Team 1 Extras conceded in_wides_No Balls</th>\n",
       "      <th>Winner (team 1=1, team 2=0)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Koramangala Traffic Jammers</td>\n",
       "      <td>Whitefield Water Loggers</td>\n",
       "      <td>Whitefield</td>\n",
       "      <td>1</td>\n",
       "      <td>01-01-2012</td>\n",
       "      <td>20:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>120.00</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Electronic City Power Savers</td>\n",
       "      <td>Silkboard Slow Movers</td>\n",
       "      <td>Silkboard</td>\n",
       "      <td>2</td>\n",
       "      <td>01-02-2012</td>\n",
       "      <td>17:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>215.15</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Indranagar Pub Watchers</td>\n",
       "      <td>Sarjapur Water Tankers</td>\n",
       "      <td>Sarjapur</td>\n",
       "      <td>3</td>\n",
       "      <td>01-03-2012</td>\n",
       "      <td>20:30:00</td>\n",
       "      <td>11</td>\n",
       "      <td>0.23</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>300.00</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Bellandur Froth Fighters</td>\n",
       "      <td>Koramangala Traffic Jammers</td>\n",
       "      <td>Koramangala</td>\n",
       "      <td>4</td>\n",
       "      <td>01-04-2012</td>\n",
       "      <td>16:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>100.00</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Marathalli Chokers</td>\n",
       "      <td>Whitefield Water Loggers</td>\n",
       "      <td>Marathalli</td>\n",
       "      <td>5</td>\n",
       "      <td>01-05-2012</td>\n",
       "      <td>20:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>0.56</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>205.26</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Game ID                        Team 1                       Team 2  \\\n",
       "0        1   Koramangala Traffic Jammers    Whitefield Water Loggers    \n",
       "1        2  Electronic City Power Savers        Silkboard Slow Movers   \n",
       "2        3       Indranagar Pub Watchers       Sarjapur Water Tankers   \n",
       "3        4      Bellandur Froth Fighters  Koramangala Traffic Jammers   \n",
       "4        5           Marathalli Chokers     Whitefield Water Loggers    \n",
       "\n",
       "          City  DayOfWeek  DateOfGame TimeOfGame  AvgWindSpeed  AvgHumidity  \\\n",
       "0   Whitefield          1  01-01-2012   20:00:00             6         0.49   \n",
       "1    Silkboard          2  01-02-2012   17:00:00             7         0.44   \n",
       "2     Sarjapur          3  01-03-2012   20:30:00            11         0.23   \n",
       "3  Koramangala          4  01-04-2012   16:00:00             6         0.61   \n",
       "4   Marathalli          5  01-05-2012   20:00:00             6         0.56   \n",
       "\n",
       "   Inn 1 Team 1 NOP R>25,SR>125             ...               \\\n",
       "0                             1             ...                \n",
       "1                             3             ...                \n",
       "2                             2             ...                \n",
       "3                             0             ...                \n",
       "4                             3             ...                \n",
       "\n",
       "   Inn 2 Team 2 NOP R<25, SR>125  Inn 2 Team 2 Total 4s  \\\n",
       "0                              0                      3   \n",
       "1                              2                     18   \n",
       "2                              1                     18   \n",
       "3                              0                      5   \n",
       "4                              3                     15   \n",
       "\n",
       "   Inn 2 Team 2 Total 6s  Inn 2 Team 2 Max Strike Rate_ALLBatsmen  \\\n",
       "0                      3                                   120.00   \n",
       "1                      9                                   215.15   \n",
       "2                      1                                   300.00   \n",
       "3                      4                                   100.00   \n",
       "4                      6                                   205.26   \n",
       "\n",
       "   Inn 2 Team 1 NoP fast bowlers  Inn 2 Team 1 NoP Spinners  \\\n",
       "0                              5                          0   \n",
       "1                              4                          1   \n",
       "2                              3                          3   \n",
       "3                              4                          2   \n",
       "4                              4                          2   \n",
       "\n",
       "   Inn 2 Team 1 wickets taken_catches_runout  \\\n",
       "0                                          6   \n",
       "1                                          4   \n",
       "2                                          0   \n",
       "3                                          3   \n",
       "4                                          4   \n",
       "\n",
       "   Inn2 Team 1 wickets taken_ bowled _lbw_caught by keeper_stumping  \\\n",
       "0                                                  4                  \n",
       "1                                                  0                  \n",
       "2                                                  1                  \n",
       "3                                                  2                  \n",
       "4                                                  1                  \n",
       "\n",
       "   Inn 2 Team 1 Extras conceded in_wides_No Balls  Winner (team 1=1, team 2=0)  \n",
       "0                                              11                            1  \n",
       "1                                               5                            1  \n",
       "2                                              10                            0  \n",
       "3                                              16                            0  \n",
       "4                                               5                            0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('IPL/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring a bit of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a total of 252 observations, 29 features, 1 label\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Game ID                                                             0\n",
       "Team 1                                                              0\n",
       "Team 2                                                              0\n",
       "City                                                                0\n",
       "DayOfWeek                                                           0\n",
       "DateOfGame                                                          0\n",
       "TimeOfGame                                                          0\n",
       "AvgWindSpeed                                                        0\n",
       "AvgHumidity                                                         0\n",
       "Inn 1 Team 1 NOP R>25,SR>125                                        0\n",
       "Inn 1 Team 1 NOP R<25, SR>125                                       0\n",
       "Inn 1 Team 1 Total 4s                                               0\n",
       "Inn 1 Team 1 Total 6s                                               0\n",
       "Inn 1 Team 1 Max Strike Rate_ALLBatsmen                             0\n",
       "Inn 1 Team 2 NoP fast bowlers                                       0\n",
       "Inn 1 Team 2 NoP Spinners                                           0\n",
       "Inn 1 Team 2 wickets taken_catches_runout                           0\n",
       "Inn1 Team 2 wickets taken_ bowled _lbw_caught by keeper_stumping    0\n",
       "Inn 1 Team 2 Extras conceded in_wides_No Balls                      0\n",
       "Inn 2 Team 2 NOP R>25,SR>125                                        0\n",
       "Inn 2 Team 2 NOP R<25, SR>125                                       0\n",
       "Inn 2 Team 2 Total 4s                                               0\n",
       "Inn 2 Team 2 Total 6s                                               0\n",
       "Inn 2 Team 2 Max Strike Rate_ALLBatsmen                             0\n",
       "Inn 2 Team 1 NoP fast bowlers                                       0\n",
       "Inn 2 Team 1 NoP Spinners                                           0\n",
       "Inn 2 Team 1 wickets taken_catches_runout                           0\n",
       "Inn2 Team 1 wickets taken_ bowled _lbw_caught by keeper_stumping    0\n",
       "Inn 2 Team 1 Extras conceded in_wides_No Balls                      0\n",
       "Winner (team 1=1, team 2=0)                                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking if there are any missing values in any of the observations\n",
    "# fortunately there is none\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    141\n",
       "1    111\n",
       "Name: Winner (team 1=1, team 2=0), dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the data is slightly unbalanced, we can ignore it for now\n",
    "df['Winner (team 1=1, team 2=0)'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 252 entries, 0 to 251\n",
      "Data columns (total 30 columns):\n",
      "Game ID                                                             252 non-null int64\n",
      "Team 1                                                              252 non-null object\n",
      "Team 2                                                              252 non-null object\n",
      "City                                                                252 non-null object\n",
      "DayOfWeek                                                           252 non-null int64\n",
      "DateOfGame                                                          252 non-null object\n",
      "TimeOfGame                                                          252 non-null object\n",
      "AvgWindSpeed                                                        252 non-null int64\n",
      "AvgHumidity                                                         252 non-null float64\n",
      "Inn 1 Team 1 NOP R>25,SR>125                                        252 non-null int64\n",
      "Inn 1 Team 1 NOP R<25, SR>125                                       252 non-null int64\n",
      "Inn 1 Team 1 Total 4s                                               252 non-null int64\n",
      "Inn 1 Team 1 Total 6s                                               252 non-null int64\n",
      "Inn 1 Team 1 Max Strike Rate_ALLBatsmen                             252 non-null float64\n",
      "Inn 1 Team 2 NoP fast bowlers                                       252 non-null int64\n",
      "Inn 1 Team 2 NoP Spinners                                           252 non-null int64\n",
      "Inn 1 Team 2 wickets taken_catches_runout                           252 non-null int64\n",
      "Inn1 Team 2 wickets taken_ bowled _lbw_caught by keeper_stumping    252 non-null int64\n",
      "Inn 1 Team 2 Extras conceded in_wides_No Balls                      252 non-null int64\n",
      "Inn 2 Team 2 NOP R>25,SR>125                                        252 non-null int64\n",
      "Inn 2 Team 2 NOP R<25, SR>125                                       252 non-null int64\n",
      "Inn 2 Team 2 Total 4s                                               252 non-null int64\n",
      "Inn 2 Team 2 Total 6s                                               252 non-null int64\n",
      "Inn 2 Team 2 Max Strike Rate_ALLBatsmen                             252 non-null float64\n",
      "Inn 2 Team 1 NoP fast bowlers                                       252 non-null int64\n",
      "Inn 2 Team 1 NoP Spinners                                           252 non-null int64\n",
      "Inn 2 Team 1 wickets taken_catches_runout                           252 non-null int64\n",
      "Inn2 Team 1 wickets taken_ bowled _lbw_caught by keeper_stumping    252 non-null int64\n",
      "Inn 2 Team 1 Extras conceded in_wides_No Balls                      252 non-null int64\n",
      "Winner (team 1=1, team 2=0)                                         252 non-null int64\n",
      "dtypes: float64(3), int64(22), object(5)\n",
      "memory usage: 59.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# we can see that Team 1, Team 2, City, DayOfWeek, DateOfGame, TimeOfGame are all strings and need to be converted\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with a bit of preprocessing where we convert the `object` dtypes into numerical categories, for 'Team 1', 'Team 2', 'City', 'TimeOfGame', 'DateOfGame'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_session = [] # 0 represents night, 1 represents evening, 2 represents afternoon extracted from `TimeOfGame`\n",
    "dates, months, years = [], [], [] # getting the date, month and year out of the `DateOfGame` strings\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    match_time = row['TimeOfGame'].split(':')\n",
    "    \n",
    "    if int(match_time[0]) >= 20:\n",
    "        time_session.append(0)\n",
    "    elif int(match_time[0]) >= 15 and int(match_time[0]) < 20:\n",
    "        time_session.append(1)\n",
    "    else:\n",
    "        time_session.append(2)\n",
    "    \n",
    "    match_date = row['DateOfGame'].split('-')\n",
    "    months.append(match_date[0])\n",
    "    dates.append(match_date[1])\n",
    "    years.append(match_date[2])\n",
    "    \n",
    "df['time_session'] = time_session\n",
    "df['months'] = months\n",
    "df['dates'] = dates\n",
    "df['years'] = years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f96f50b0fd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot of the above mentioned numbers, it's gaining a lot of traction over the years\n",
    "df['years'].value_counts().plot(kind='bar', x='year', y='matches conducted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f96f14fd390>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD+CAYAAAA09s7qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADWlJREFUeJzt3X+s3XV9x/Hny95VROMK9sJqy1a2NTr8sUFuGBvJYuySgTrLH5JAFlaVpFuCm45l/NiWoFlMwC3TLZkmDaA1IWjDXGg294NUnNkfIBdEfhVsg7PctaPXKMzNZFp874/7rbvW+6P3fM/p6f30+UjIPd8f537f5Bue98v3nnNPqgpJUrteNu4BJEmjZeglqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaNzHuAQDWr19fmzdvHvcYkrSqPPzww9+sqsnl9jslQr9582amp6fHPYYkrSpJvnEi+3nrRpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGGXpIaZ+glqXGnxBumTrbNN/3DuEcYqX+/9e3jHkHSKcQreklqnKGXpMYZeklq3LKhT3JnkiNJnpi37s+TPJ3ksSR/l2TdvG03JzmQ5JkkvzGqwSVJJ+ZErug/BVx23Lr7gDdW1ZuBrwE3AyS5ALgKeEP3nI8nWTO0aSVJK7Zs6KvqS8C3jlv3L1V1tFt8ANjUPd4GfKaq/reqvg4cAC4e4rySpBUaxj369wL/2D3eCDw3b9tMt+7HJNmRZDrJ9Ozs7BDGkCQtpFfok/wJcBS469iqBXarhZ5bVTuraqqqpiYnl/2AFEnSgAZ+w1SS7cA7gK1VdSzmM8B583bbBBwafDxJUl8DXdEnuQy4EXhnVX133qY9wFVJXp7kfGAL8OX+Y0qSBrXsFX2Su4G3AOuTzAC3MPcqm5cD9yUBeKCqfreqnkyyG3iKuVs611XVS6MaXpK0vGVDX1VXL7D6jiX2/zDw4T5DSZKGx3fGSlLjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjTsvPjNUq98GfHPcEo/XBF8c9gRrjFb0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1Ljlg19kjuTHEnyxLx1Zye5L8n+7utZ3fok+eskB5I8luSiUQ4vSVreiVzRfwq47Lh1NwF7q2oLsLdbBrgc2NL9swP4xHDGlCQNatnQV9WXgG8dt3obsKt7vAu4Yt76T9ecB4B1STYMa1hJ0soNeo/+3Ko6DNB9PadbvxF4bt5+M926H5NkR5LpJNOzs7MDjiFJWs6wfxmbBdbVQjtW1c6qmqqqqcnJySGPIUk6ZtDQP3/slkz39Ui3fgY4b95+m4BDg48nSepr0NDvAbZ3j7cD985b/9vdq28uAV48dotHkjQeE8vtkORu4C3A+iQzwC3ArcDuJNcCB4Eru90/D7wNOAB8F3jPCGaWJK3AsqGvqqsX2bR1gX0LuK7vUJKk4fGdsZLUOEMvSY0z9JLUOEMvSY0z9JLUOEMvSY0z9JLUOEMvSY0z9JLUOEMvSY0z9JLUOEMvSY0z9JLUOEMvSY0z9JLUOEMvSY0z9JLUOEMvSY0z9JLUOEMvSY0z9JLUOEMvSY0z9JLUOEMvSY0z9JLUuF6hT/IHSZ5M8kSSu5OckeT8JA8m2Z/ks0nWDmtYSdLKDRz6JBuB3wemquqNwBrgKuA24KNVtQX4NnDtMAaVJA2m762bCeAVSSaAM4HDwFuBe7rtu4Areh5DktTDwKGvqv8A/gI4yFzgXwQeBl6oqqPdbjPAxoWen2RHkukk07Ozs4OOIUlaRp9bN2cB24DzgdcCrwQuX2DXWuj5VbWzqqaqampycnLQMSRJy+hz6+bXga9X1WxVfR/4HPCrwLruVg7AJuBQzxklST30Cf1B4JIkZyYJsBV4CrgfeFe3z3bg3n4jSpL66HOP/kHmfun6CPB49712AjcC1yc5ALwGuGMIc0qSBjSx/C6Lq6pbgFuOW/0scHGf7ytJGh7fGStJjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktS4XqFPsi7JPUmeTrIvya8kOTvJfUn2d1/PGtawkqSV63tF/1fAP1XV64FfBPYBNwF7q2oLsLdbliSNycChT/Jq4NeAOwCq6ntV9QKwDdjV7bYLuKLvkJKkwfW5ov9ZYBb4ZJKvJLk9ySuBc6vqMED39ZyFnpxkR5LpJNOzs7M9xpAkLaVP6CeAi4BPVNWFwP+wgts0VbWzqqaqampycrLHGJKkpfQJ/QwwU1UPdsv3MBf+55NsAOi+Huk3oiSpj4FDX1X/CTyX5HXdqq3AU8AeYHu3bjtwb68JJUm9TPR8/u8BdyVZCzwLvIe5Hx67k1wLHASu7HkMSVIPvUJfVY8CUwts2trn+0qShsd3xkpS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDWud+iTrEnylSR/3y2fn+TBJPuTfDbJ2v5jSpIGNYwr+vcD++Yt3wZ8tKq2AN8Grh3CMSRJA+oV+iSbgLcDt3fLAd4K3NPtsgu4os8xJEn99L2i/xhwA/CDbvk1wAtVdbRbngE2LvTEJDuSTCeZnp2d7TmGJGkxA4c+yTuAI1X18PzVC+xaCz2/qnZW1VRVTU1OTg46hiRpGRM9nnsp8M4kbwPOAF7N3BX+uiQT3VX9JuBQ/zElSYMa+Iq+qm6uqk1VtRm4CvhCVf0WcD/wrm637cC9vaeUJA1sFK+jvxG4PskB5u7Z3zGCY0iSTlCfWzc/VFVfBL7YPX4WuHgY31eS1J/vjJWkxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxhl6SWqcoZekxg0c+iTnJbk/yb4kTyZ5f7f+7CT3JdnffT1reONKklaqzxX9UeAPq+oXgEuA65JcANwE7K2qLcDeblmSNCYDh76qDlfVI93j7wD7gI3ANmBXt9su4Iq+Q0qSBjeUe/RJNgMXAg8C51bVYZj7YQCcs8hzdiSZTjI9Ozs7jDEkSQvoHfokrwL+FvhAVf3XiT6vqnZW1VRVTU1OTvYdQ5K0iF6hT/ITzEX+rqr6XLf6+SQbuu0bgCP9RpQk9dHnVTcB7gD2VdVfztu0B9jePd4O3Dv4eJKkviZ6PPdS4Brg8SSPduv+GLgV2J3kWuAgcGW/ESVJfQwc+qr6NyCLbN466PeVJA1Xnyt6SVqRN+1607hHGKnHtz8+7hEW5J9AkKTGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGGXpJapyhl6TGjSz0SS5L8kySA0luGtVxJElLG0nok6wB/ga4HLgAuDrJBaM4liRpaaO6or8YOFBVz1bV94DPANtGdCxJ0hImRvR9NwLPzVueAX55/g5JdgA7usX/TvLMiGY5FawHvnmyDpbbTtaRThsn9fzxoZy0Q50GTu5/e+8+6efuZ05kp1GFfqF/2/qRhaqdwM4RHf+UkmS6qqbGPYcG4/lbvTx3c0Z162YGOG/e8ibg0IiOJUlawqhC/xCwJcn5SdYCVwF7RnQsSdISRnLrpqqOJnkf8M/AGuDOqnpyFMdaJU6LW1QN8/ytXp47IFW1/F6SpFXLd8ZKUuMMvSQ1ztBLUuMMvSQ1ztBLUuMM/UmUxJd6ncKSrEnyO0n+LMmlx23703HNpROT5MwkNyT5oyRnJHl3kj1JPpLkVeOeb5x8eeWQJTl7sU3AV6tq08mcRycuye3AmcCXgWuAf62q67ttj1TVReOcT0tLspu5v7H1CuB1wD5gN/CbwE9V1TVjHG+sDP2QJXkJ+AY/+vd+qlveWFVrxzKYlpXksap6c/d4Avg4c38U62rggaq6cJzzaWlJHq2qX0oS4DCwoaqqW/7qsXN7OhrVHzU7nT0LbK2qg8dvSPLcAvvr1PHDH8JVdRTYkeQW4AvAaf2//qtJF/fPV3cV2y2f1le03qMfvo8BZy2y7SMncxCt2HSSy+avqKoPAZ8ENo9lIq3E9LF78VX13mMrk/wc8J2xTXUK8NbNCCR5PXMftLKRuds2h4A9VbVvrINpWZ671W2x8wc8Xadx7LyiH7IkNzD3iVph7pd6D3WP7/azc09tnrvVbanzB9w4xtHGziv6IUvyNeANVfX949avBZ6sqi3jmUzL8dytbp6/xXlFP3w/AF67wPoN3Tadujx3q5vnbxG+6mb4PgDsTbKf///c3J8Gfh5439im0onw3K1unr9FeOtmBJK8DLiYuV8IhbmPVnyoql4a62BalududfP8LczQS1LjvEcvSY0z9JLUOEMvSY0z9JLUuP8D/NYeCbvGBdEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# apparently matches are conducted only during Quarter 1 of every year.\n",
    "df['months'].value_counts().plot(kind='bar', x='month', y='matches conducted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Marathalli         47\n",
       "Sarjapur           40\n",
       "Koramangala        38\n",
       "Whitefield         33\n",
       "Indranagar         27\n",
       "Electronic City    25\n",
       "Silkboard          25\n",
       "Bellandur          17\n",
       "Name: City, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are 8 cities for the matches\n",
    "df['City'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import bisect\n",
    "city_le = LabelEncoder()\n",
    "team_name_le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reading the test file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring a bit of the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Game ID</th>\n",
       "      <th>Team 1</th>\n",
       "      <th>Team 2</th>\n",
       "      <th>CityOfGame</th>\n",
       "      <th>Day</th>\n",
       "      <th>DateOfGame</th>\n",
       "      <th>TimeOfGame</th>\n",
       "      <th>AvgWindSpeed</th>\n",
       "      <th>AvgHumidity</th>\n",
       "      <th>Inn 1 Team 1 NOP R&gt;25,SR&gt;125</th>\n",
       "      <th>...</th>\n",
       "      <th>Inn 2 Team 2 NOP R&lt;25, SR&gt;125</th>\n",
       "      <th>Inn 2 Team 2 Total 4s</th>\n",
       "      <th>Inn 2 Team 2 Total 6s</th>\n",
       "      <th>Inn 2 Team 2 Max Strike Rate_ALLBatsmen</th>\n",
       "      <th>Inn 2 Team 1 NoP fast bowlers</th>\n",
       "      <th>Inn 2 Team 1 NoP Spinners</th>\n",
       "      <th>Inn 2 Team 1 wickets taken_catches_runout</th>\n",
       "      <th>Inn2 Team 1 wickets taken_ bowled _lbw_caught by keeper_stumping</th>\n",
       "      <th>Inn 2 Team 1 Extras conceded in_wides_No Balls</th>\n",
       "      <th>Winner (team 1=1, team 2=0)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>253</td>\n",
       "      <td>Electronic City Power Savers</td>\n",
       "      <td>Marathalli Chokers</td>\n",
       "      <td>Electronic City</td>\n",
       "      <td>6</td>\n",
       "      <td>01-01-2016</td>\n",
       "      <td>20:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>142.85</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>254</td>\n",
       "      <td>Koramangala Traffic Jammers</td>\n",
       "      <td>Sarjapur Water Tankers</td>\n",
       "      <td>Koramangala</td>\n",
       "      <td>7</td>\n",
       "      <td>01-02-2016</td>\n",
       "      <td>20:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>210.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>255</td>\n",
       "      <td>HSR High Rent Payers</td>\n",
       "      <td>Marathalli Chokers</td>\n",
       "      <td>Marathalli</td>\n",
       "      <td>1</td>\n",
       "      <td>01-03-2016</td>\n",
       "      <td>16:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>166.66</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>256</td>\n",
       "      <td>Indranagar Pub Watchers</td>\n",
       "      <td>Silkboard Slow Movers</td>\n",
       "      <td>Indranagar</td>\n",
       "      <td>2</td>\n",
       "      <td>01-04-2016</td>\n",
       "      <td>20:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>166.66</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>257</td>\n",
       "      <td>Whitefield Water Loggers</td>\n",
       "      <td>Sarjapur Water Tankers</td>\n",
       "      <td>Whitefield</td>\n",
       "      <td>3</td>\n",
       "      <td>01-05-2016</td>\n",
       "      <td>16:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>0.62</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>160.00</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Game ID                        Team 1                  Team 2  \\\n",
       "0      253  Electronic City Power Savers     Marathalli Chokers    \n",
       "1      254   Koramangala Traffic Jammers  Sarjapur Water Tankers   \n",
       "2      255          HSR High Rent Payers     Marathalli Chokers    \n",
       "3      256       Indranagar Pub Watchers   Silkboard Slow Movers   \n",
       "4      257     Whitefield Water Loggers   Sarjapur Water Tankers   \n",
       "\n",
       "        CityOfGame  Day  DateOfGame TimeOfGame  AvgWindSpeed  AvgHumidity  \\\n",
       "0  Electronic City    6  01-01-2016   20:00:00             5         0.62   \n",
       "1      Koramangala    7  01-02-2016   20:00:00             6         0.66   \n",
       "2       Marathalli    1  01-03-2016   16:00:00             5         0.64   \n",
       "3       Indranagar    2  01-04-2016   20:00:00             5         0.64   \n",
       "4       Whitefield    3  01-05-2016   16:00:00             5         0.62   \n",
       "\n",
       "   Inn 1 Team 1 NOP R>25,SR>125             ...               \\\n",
       "0                             1             ...                \n",
       "1                             1             ...                \n",
       "2                             0             ...                \n",
       "3                             1             ...                \n",
       "4                             2             ...                \n",
       "\n",
       "   Inn 2 Team 2 NOP R<25, SR>125  Inn 2 Team 2 Total 4s  \\\n",
       "0                              0                      9   \n",
       "1                              1                      8   \n",
       "2                              1                      8   \n",
       "3                              3                     14   \n",
       "4                              2                     13   \n",
       "\n",
       "   Inn 2 Team 2 Total 6s  Inn 2 Team 2 Max Strike Rate_ALLBatsmen  \\\n",
       "0                      5                                   142.85   \n",
       "1                      4                                   210.00   \n",
       "2                      1                                   166.66   \n",
       "3                      4                                   166.66   \n",
       "4                      2                                   160.00   \n",
       "\n",
       "   Inn 2 Team 1 NoP fast bowlers  Inn 2 Team 1 NoP Spinners  \\\n",
       "0                              3                          3   \n",
       "1                              3                          2   \n",
       "2                              4                          2   \n",
       "3                              2                          4   \n",
       "4                              4                          2   \n",
       "\n",
       "   Inn 2 Team 1 wickets taken_catches_runout  \\\n",
       "0                                          1   \n",
       "1                                          1   \n",
       "2                                          2   \n",
       "3                                          6   \n",
       "4                                          6   \n",
       "\n",
       "   Inn2 Team 1 wickets taken_ bowled _lbw_caught by keeper_stumping  \\\n",
       "0                                                  1                  \n",
       "1                                                  1                  \n",
       "2                                                  7                  \n",
       "3                                                  3                  \n",
       "4                                                  1                  \n",
       "\n",
       "   Inn 2 Team 1 Extras conceded in_wides_No Balls  Winner (team 1=1, team 2=0)  \n",
       "0                                               3                            0  \n",
       "1                                               1                            0  \n",
       "2                                               4                            1  \n",
       "3                                               2                            1  \n",
       "4                                               2                            1  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('IPL/test.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 30)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HSR                10\n",
       "Koramangala        10\n",
       "Electronic City    10\n",
       "Whitefield          9\n",
       "Silkboard           8\n",
       "Indranagar          8\n",
       "Marathalli          8\n",
       "Sarjapur            8\n",
       "Bellandur           5\n",
       "Name: CityOfGame, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can see that test dataset contains a new category called 'HSR' which is not present in the training dataset\n",
    "df_test['CityOfGame'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Electronic City Power Savers    13\n",
       "Bellandur Froth Fighters        12\n",
       "Indranagar Pub Watchers         11\n",
       "Silkboard Slow Movers            8\n",
       "Koramangala Traffic Jammers      8\n",
       "Whitefield Water Loggers         7\n",
       "HSR High Rent Payers             7\n",
       "Marathalli Chokers               6\n",
       "Sarjapur Water Tankers           4\n",
       "Name: Team 1, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# two teams from the train are missing but that isn't a problem,\n",
    "# but a new team has been included in the test dataset but not in train dataset: 'HSR High Rent Payers'\n",
    "df_test['Team 1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_session = [] # 0 represents night, 1 represents evening, 2 represents afternoon extracted from `TimeOfGame`\n",
    "dates, months, years = [], [], [] # getting the date, month and year out of the `DateOfGame` strings\n",
    "\n",
    "for index, row in df_test.iterrows():\n",
    "    \n",
    "    match_time = row['TimeOfGame'].split(':')\n",
    "    \n",
    "    if int(match_time[0]) >= 20:\n",
    "        time_session.append(0)\n",
    "    elif int(match_time[0]) >= 15 and int(match_time[0]) < 20:\n",
    "        time_session.append(1)\n",
    "    else:\n",
    "        time_session.append(2)\n",
    "    \n",
    "    match_date = row['DateOfGame'].split('-')\n",
    "    months.append(match_date[0])\n",
    "    dates.append(match_date[1])\n",
    "    years.append(match_date[2])\n",
    "    \n",
    "df_test['time_session'] = time_session\n",
    "df_test['months'] = months\n",
    "df_test['dates'] = dates\n",
    "df_test['years'] = years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't be optimistic and think that there won't be new teams or new stadiums in the future, therefore it is important to be considerate and include an `other` category in the categorical columns along with other categories present in `train.csv`, this ensures even when unseen data is thrown for any of the following three fields: `Team 1`, `Team 2` and `City`/`CityOfGame`, it can handle it can generalize well and make a bearable prediction at the very least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fitting team names from train.csv \n",
    "team_name_le.fit(df['Team 1'])\n",
    "\n",
    "# mapping all the unseen team name categories from test.csv to 'other' i.e., HSR High Rent Payers\n",
    "df_test['Team 1'] = df_test['Team 1'].map(lambda s: 'other' if s not in team_name_le.classes_ else s)\n",
    "df_test['Team 2'] = df_test['Team 2'].map(lambda s: 'other' if s not in team_name_le.classes_ else s)\n",
    "\n",
    "# we include 'other' in the classes for team names\n",
    "team_name_le_classes = team_name_le.classes_.tolist()\n",
    "bisect.insort_left(team_name_le_classes, 'other')\n",
    "team_name_le.classes_ = team_name_le_classes\n",
    "\n",
    "# now it has an additional class and we can transform the train and test accordingly.\n",
    "df['team_1_encoded'] = team_name_le.transform(df['Team 1'])\n",
    "df_test['team_1_encoded'] = team_name_le.transform(df_test['Team 1'])\n",
    "\n",
    "df['team_2_encoded'] = team_name_le.transform(df['Team 2'])\n",
    "df_test['team_2_encoded'] = team_name_le.transform(df_test['Team 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Airport Flyers',\n",
       " 'Bellandur Froth Fighters',\n",
       " 'Electronic City Power Savers',\n",
       " 'Forum Fans',\n",
       " 'Indranagar Pub Watchers',\n",
       " 'Koramangala Traffic Jammers',\n",
       " 'Marathalli Chokers ',\n",
       " 'Sarjapur Water Tankers',\n",
       " 'Silkboard Slow Movers',\n",
       " 'Whitefield Water Loggers ',\n",
       " 'other']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HSR high rent payers has been converted into other, this is how dataset will handle new team names and cities\n",
    "team_name_le_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting city names from train.csv \n",
    "city_le.fit(df['City'])\n",
    "\n",
    "# mapping all the unseen team name categories from test.csv to 'other' i.e., 'HSR' \n",
    "df_test['CityOfGame'] = df_test['CityOfGame'].map(lambda s: 'other' if s not in city_le.classes_ else s)\n",
    "\n",
    "# we include 'other' in the classes for city names\n",
    "city_le_classes = city_le.classes_.tolist()\n",
    "bisect.insort_left(city_le_classes, 'other')\n",
    "city_le.classes_ = city_le_classes\n",
    "\n",
    "# now it has an additional class and we can transform the train and test accordingly.\n",
    "df['city_encoded'] = city_le.transform(df['City'])\n",
    "df_test['city_encoded'] = city_le.transform(df_test['CityOfGame'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bellandur',\n",
       " 'Electronic City',\n",
       " 'Indranagar',\n",
       " 'Koramangala',\n",
       " 'Marathalli',\n",
       " 'Sarjapur',\n",
       " 'Silkboard',\n",
       " 'Whitefield',\n",
       " 'other']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_le.classes_ # HSR in test df is being considered as 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['City', 'city_encoded']]  # uncomment this and next cell to verify they mapped fine or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test[['CityOfGame', 'city_encoded']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['Team 1', 'team_1_encoded']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test[['Team 2', 'team_2_encoded']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Team 1', 'Team 2', 'City', 'Game ID', 'TimeOfGame', 'DateOfGame'], axis = 1, inplace = True)\n",
    "df_test.drop(['Team 1', 'Team 2', 'CityOfGame', 'Game ID', 'TimeOfGame', 'DateOfGame'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>AvgWindSpeed</th>\n",
       "      <th>AvgHumidity</th>\n",
       "      <th>Inn 1 Team 1 NOP R&gt;25,SR&gt;125</th>\n",
       "      <th>Inn 1 Team 1 NOP R&lt;25, SR&gt;125</th>\n",
       "      <th>Inn 1 Team 1 Total 4s</th>\n",
       "      <th>Inn 1 Team 1 Total 6s</th>\n",
       "      <th>Inn 1 Team 1 Max Strike Rate_ALLBatsmen</th>\n",
       "      <th>Inn 1 Team 2 NoP fast bowlers</th>\n",
       "      <th>Inn 1 Team 2 NoP Spinners</th>\n",
       "      <th>...</th>\n",
       "      <th>Inn2 Team 1 wickets taken_ bowled _lbw_caught by keeper_stumping</th>\n",
       "      <th>Inn 2 Team 1 Extras conceded in_wides_No Balls</th>\n",
       "      <th>Winner (team 1=1, team 2=0)</th>\n",
       "      <th>time_session</th>\n",
       "      <th>months</th>\n",
       "      <th>dates</th>\n",
       "      <th>years</th>\n",
       "      <th>team_1_encoded</th>\n",
       "      <th>team_2_encoded</th>\n",
       "      <th>city_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>216.43</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>01</td>\n",
       "      <td>01</td>\n",
       "      <td>2012</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.44</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>246.15</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>01</td>\n",
       "      <td>02</td>\n",
       "      <td>2012</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0.23</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>200.00</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>01</td>\n",
       "      <td>03</td>\n",
       "      <td>2012</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>140.00</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>01</td>\n",
       "      <td>04</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.56</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>233.33</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>01</td>\n",
       "      <td>05</td>\n",
       "      <td>2012</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   DayOfWeek  AvgWindSpeed  AvgHumidity  Inn 1 Team 1 NOP R>25,SR>125  \\\n",
       "0          1             6         0.49                             1   \n",
       "1          2             7         0.44                             3   \n",
       "2          3            11         0.23                             2   \n",
       "3          4             6         0.61                             0   \n",
       "4          5             6         0.56                             3   \n",
       "\n",
       "   Inn 1 Team 1 NOP R<25, SR>125  Inn 1 Team 1 Total 4s  \\\n",
       "0                              1                     15   \n",
       "1                              2                     20   \n",
       "2                              2                     13   \n",
       "3                              2                      6   \n",
       "4                              1                     18   \n",
       "\n",
       "   Inn 1 Team 1 Total 6s  Inn 1 Team 1 Max Strike Rate_ALLBatsmen  \\\n",
       "0                     14                                   216.43   \n",
       "1                     16                                   246.15   \n",
       "2                      3                                   200.00   \n",
       "3                      6                                   140.00   \n",
       "4                      5                                   233.33   \n",
       "\n",
       "   Inn 1 Team 2 NoP fast bowlers  Inn 1 Team 2 NoP Spinners      ...       \\\n",
       "0                              4                          2      ...        \n",
       "1                              5                          2      ...        \n",
       "2                              4                          1      ...        \n",
       "3                              3                          3      ...        \n",
       "4                              5                          1      ...        \n",
       "\n",
       "   Inn2 Team 1 wickets taken_ bowled _lbw_caught by keeper_stumping  \\\n",
       "0                                                  4                  \n",
       "1                                                  0                  \n",
       "2                                                  1                  \n",
       "3                                                  2                  \n",
       "4                                                  1                  \n",
       "\n",
       "   Inn 2 Team 1 Extras conceded in_wides_No Balls  \\\n",
       "0                                              11   \n",
       "1                                               5   \n",
       "2                                              10   \n",
       "3                                              16   \n",
       "4                                               5   \n",
       "\n",
       "   Winner (team 1=1, team 2=0)  time_session  months  dates  years  \\\n",
       "0                            1             0      01     01   2012   \n",
       "1                            1             1      01     02   2012   \n",
       "2                            0             0      01     03   2012   \n",
       "3                            0             1      01     04   2012   \n",
       "4                            0             0      01     05   2012   \n",
       "\n",
       "   team_1_encoded  team_2_encoded  city_encoded  \n",
       "0               5               9             7  \n",
       "1               2               8             6  \n",
       "2               4               7             5  \n",
       "3               1               5             3  \n",
       "4               6               9             4  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into features and targets\n",
    "X = df.drop(['Winner (team 1=1, team 2=0)'], axis = 1)\n",
    "y = df['Winner (team 1=1, team 2=0)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking 80% of the data for training and rest for validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, make_scorer, classification_report, \\\n",
    "roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the values are normalized within the range of 0 to 1\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12984713, 0.10910254, 0.08298632, 0.07494916, 0.06881151,\n",
       "       0.06151   , 0.05812985, 0.0520743 , 0.04525569, 0.04041722])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying to get the top 10 varying principal components\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(X_train_scaled)\n",
    "\n",
    "X_train_pca10 = pca.transform(X_train_scaled)\n",
    "X_valid_pca10 = pca.transform(X_valid_scaled)\n",
    "\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65730944, 0.3208923 , 0.00854764])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying to get the top 3 varying principal components\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(X_train)\n",
    "\n",
    "X_train_pca3 = pca.transform(X_train_scaled)\n",
    "X_valid_pca3 = pca.transform(X_valid_scaled)\n",
    "\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# picking only the top 50% contributing/significant features out of all the features based on ANOVA\n",
    "X_train_reduce50 = SelectPercentile(percentile=50).fit_transform(X_train_scaled, y_train)\n",
    "X_valid_reduce50 = SelectPercentile(percentile=50).fit_transform(X_valid_scaled, y_valid)\n",
    "\n",
    "# picking only the top 10% contributing/significant features out of all the features based on ANOVA\n",
    "X_train_reduce10 = SelectPercentile().fit_transform(X_train_scaled, y_train)\n",
    "X_valid_reduce10 = SelectPercentile().fit_transform(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MeanAccuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1Score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AdaBoost Classifier</th>\n",
       "      <td>0.816111</td>\n",
       "      <td>0.836050</td>\n",
       "      <td>0.856197</td>\n",
       "      <td>0.845919</td>\n",
       "      <td>0.807507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost Classifier (10% of features)</th>\n",
       "      <td>0.845745</td>\n",
       "      <td>0.872679</td>\n",
       "      <td>0.872650</td>\n",
       "      <td>0.870417</td>\n",
       "      <td>0.839543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost Classifier (50% of features)</th>\n",
       "      <td>0.830597</td>\n",
       "      <td>0.841500</td>\n",
       "      <td>0.880769</td>\n",
       "      <td>0.858853</td>\n",
       "      <td>0.819573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost Classifier (PCA: 10 components)</th>\n",
       "      <td>0.865428</td>\n",
       "      <td>0.890481</td>\n",
       "      <td>0.880769</td>\n",
       "      <td>0.884277</td>\n",
       "      <td>0.862122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost Classifier (PCA: 3 components)</th>\n",
       "      <td>0.606610</td>\n",
       "      <td>0.655234</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.673993</td>\n",
       "      <td>0.588183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost Classifier (scaled data)</th>\n",
       "      <td>0.816111</td>\n",
       "      <td>0.836050</td>\n",
       "      <td>0.856197</td>\n",
       "      <td>0.845919</td>\n",
       "      <td>0.807507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree Classifier</th>\n",
       "      <td>0.810990</td>\n",
       "      <td>0.814144</td>\n",
       "      <td>0.838889</td>\n",
       "      <td>0.823437</td>\n",
       "      <td>0.792674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree Classifier (10% of features)</th>\n",
       "      <td>0.845150</td>\n",
       "      <td>0.855814</td>\n",
       "      <td>0.889744</td>\n",
       "      <td>0.868386</td>\n",
       "      <td>0.835524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree Classifier (50% of features)</th>\n",
       "      <td>0.781726</td>\n",
       "      <td>0.807428</td>\n",
       "      <td>0.855769</td>\n",
       "      <td>0.819322</td>\n",
       "      <td>0.769794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree Classifier (PCA: 10 components)</th>\n",
       "      <td>0.776315</td>\n",
       "      <td>0.845115</td>\n",
       "      <td>0.796154</td>\n",
       "      <td>0.831298</td>\n",
       "      <td>0.769421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree Classifier (PCA: 3 components)</th>\n",
       "      <td>0.582325</td>\n",
       "      <td>0.637117</td>\n",
       "      <td>0.660470</td>\n",
       "      <td>0.655667</td>\n",
       "      <td>0.535375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree Classifier (scaled data)</th>\n",
       "      <td>0.766134</td>\n",
       "      <td>0.800570</td>\n",
       "      <td>0.864316</td>\n",
       "      <td>0.825941</td>\n",
       "      <td>0.778549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes</th>\n",
       "      <td>0.840550</td>\n",
       "      <td>0.898305</td>\n",
       "      <td>0.821581</td>\n",
       "      <td>0.857743</td>\n",
       "      <td>0.844433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes (10% of features)</th>\n",
       "      <td>0.875527</td>\n",
       "      <td>0.913790</td>\n",
       "      <td>0.872650</td>\n",
       "      <td>0.892076</td>\n",
       "      <td>0.875919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes (50% of features)</th>\n",
       "      <td>0.865503</td>\n",
       "      <td>0.926857</td>\n",
       "      <td>0.838462</td>\n",
       "      <td>0.879274</td>\n",
       "      <td>0.871171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes (PCA: 10 components)</th>\n",
       "      <td>0.900557</td>\n",
       "      <td>0.913231</td>\n",
       "      <td>0.923291</td>\n",
       "      <td>0.915586</td>\n",
       "      <td>0.895287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes (PCA: 3 components)</th>\n",
       "      <td>0.621611</td>\n",
       "      <td>0.644841</td>\n",
       "      <td>0.804274</td>\n",
       "      <td>0.708947</td>\n",
       "      <td>0.584015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian Naive Bayes (scaled data)</th>\n",
       "      <td>0.840550</td>\n",
       "      <td>0.898305</td>\n",
       "      <td>0.821581</td>\n",
       "      <td>0.857743</td>\n",
       "      <td>0.844433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Discriminant Analysis</th>\n",
       "      <td>0.900260</td>\n",
       "      <td>0.916441</td>\n",
       "      <td>0.914957</td>\n",
       "      <td>0.914979</td>\n",
       "      <td>0.896853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Discriminant Analysis (10% of features)</th>\n",
       "      <td>0.865426</td>\n",
       "      <td>0.898544</td>\n",
       "      <td>0.872650</td>\n",
       "      <td>0.884654</td>\n",
       "      <td>0.863573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Discriminant Analysis (50% of features)</th>\n",
       "      <td>0.900409</td>\n",
       "      <td>0.926002</td>\n",
       "      <td>0.906197</td>\n",
       "      <td>0.913924</td>\n",
       "      <td>0.898865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Discriminant Analysis (PCA: 10 components)</th>\n",
       "      <td>0.890603</td>\n",
       "      <td>0.932323</td>\n",
       "      <td>0.881197</td>\n",
       "      <td>0.905044</td>\n",
       "      <td>0.892318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Discriminant Analysis (PCA: 3 components)</th>\n",
       "      <td>0.656592</td>\n",
       "      <td>0.651139</td>\n",
       "      <td>0.897863</td>\n",
       "      <td>0.753714</td>\n",
       "      <td>0.606119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Discriminant Analysis (scaled data)</th>\n",
       "      <td>0.900260</td>\n",
       "      <td>0.916441</td>\n",
       "      <td>0.914957</td>\n",
       "      <td>0.914979</td>\n",
       "      <td>0.896853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.895358</td>\n",
       "      <td>0.902952</td>\n",
       "      <td>0.923291</td>\n",
       "      <td>0.911746</td>\n",
       "      <td>0.889114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression (10% of features)</th>\n",
       "      <td>0.875451</td>\n",
       "      <td>0.906231</td>\n",
       "      <td>0.881197</td>\n",
       "      <td>0.893122</td>\n",
       "      <td>0.874020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression (50% of features)</th>\n",
       "      <td>0.905235</td>\n",
       "      <td>0.923947</td>\n",
       "      <td>0.914744</td>\n",
       "      <td>0.918516</td>\n",
       "      <td>0.903139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression (PCA: 10 components)</th>\n",
       "      <td>0.880575</td>\n",
       "      <td>0.900637</td>\n",
       "      <td>0.898291</td>\n",
       "      <td>0.899071</td>\n",
       "      <td>0.876614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression (PCA: 3 components)</th>\n",
       "      <td>0.602081</td>\n",
       "      <td>0.597002</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>0.745238</td>\n",
       "      <td>0.519977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression (scaled data)</th>\n",
       "      <td>0.915263</td>\n",
       "      <td>0.925363</td>\n",
       "      <td>0.931624</td>\n",
       "      <td>0.927508</td>\n",
       "      <td>0.911800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Classifier</th>\n",
       "      <td>0.790865</td>\n",
       "      <td>0.809829</td>\n",
       "      <td>0.889530</td>\n",
       "      <td>0.854762</td>\n",
       "      <td>0.830608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Classifier (10% of features)</th>\n",
       "      <td>0.855475</td>\n",
       "      <td>0.863882</td>\n",
       "      <td>0.906838</td>\n",
       "      <td>0.863169</td>\n",
       "      <td>0.811822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Classifier (50% of features)</th>\n",
       "      <td>0.875156</td>\n",
       "      <td>0.847446</td>\n",
       "      <td>0.898077</td>\n",
       "      <td>0.867908</td>\n",
       "      <td>0.841495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Classifier (PCA: 10 components)</th>\n",
       "      <td>0.806167</td>\n",
       "      <td>0.788286</td>\n",
       "      <td>0.864530</td>\n",
       "      <td>0.840541</td>\n",
       "      <td>0.784481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Classifier (PCA: 3 components)</th>\n",
       "      <td>0.637204</td>\n",
       "      <td>0.626672</td>\n",
       "      <td>0.728632</td>\n",
       "      <td>0.680917</td>\n",
       "      <td>0.584224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest Classifier (scaled data)</th>\n",
       "      <td>0.821015</td>\n",
       "      <td>0.766398</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.877445</td>\n",
       "      <td>0.837556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Machine</th>\n",
       "      <td>0.587078</td>\n",
       "      <td>0.587078</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.739816</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Machine (10% of features)</th>\n",
       "      <td>0.875451</td>\n",
       "      <td>0.906231</td>\n",
       "      <td>0.881197</td>\n",
       "      <td>0.893122</td>\n",
       "      <td>0.874020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Machine (50% of features)</th>\n",
       "      <td>0.905235</td>\n",
       "      <td>0.902284</td>\n",
       "      <td>0.940385</td>\n",
       "      <td>0.920732</td>\n",
       "      <td>0.897882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Machine (PCA: 10 components)</th>\n",
       "      <td>0.875598</td>\n",
       "      <td>0.914182</td>\n",
       "      <td>0.872863</td>\n",
       "      <td>0.892510</td>\n",
       "      <td>0.876026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Machine (PCA: 3 components)</th>\n",
       "      <td>0.592128</td>\n",
       "      <td>0.590108</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.742197</td>\n",
       "      <td>0.506173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Machine (scaled data)</th>\n",
       "      <td>0.757148</td>\n",
       "      <td>0.717507</td>\n",
       "      <td>0.991453</td>\n",
       "      <td>0.830069</td>\n",
       "      <td>0.708028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   MeanAccuracy  Precision  \\\n",
       "AdaBoost Classifier                                    0.816111   0.836050   \n",
       "AdaBoost Classifier (10% of features)                  0.845745   0.872679   \n",
       "AdaBoost Classifier (50% of features)                  0.830597   0.841500   \n",
       "AdaBoost Classifier (PCA: 10 components)               0.865428   0.890481   \n",
       "AdaBoost Classifier (PCA: 3 components)                0.606610   0.655234   \n",
       "AdaBoost Classifier (scaled data)                      0.816111   0.836050   \n",
       "Decision Tree Classifier                               0.810990   0.814144   \n",
       "Decision Tree Classifier (10% of features)             0.845150   0.855814   \n",
       "Decision Tree Classifier (50% of features)             0.781726   0.807428   \n",
       "Decision Tree Classifier (PCA: 10 components)          0.776315   0.845115   \n",
       "Decision Tree Classifier (PCA: 3 components)           0.582325   0.637117   \n",
       "Decision Tree Classifier (scaled data)                 0.766134   0.800570   \n",
       "Gaussian Naive Bayes                                   0.840550   0.898305   \n",
       "Gaussian Naive Bayes (10% of features)                 0.875527   0.913790   \n",
       "Gaussian Naive Bayes (50% of features)                 0.865503   0.926857   \n",
       "Gaussian Naive Bayes (PCA: 10 components)              0.900557   0.913231   \n",
       "Gaussian Naive Bayes (PCA: 3 components)               0.621611   0.644841   \n",
       "Gaussian Naive Bayes (scaled data)                     0.840550   0.898305   \n",
       "Linear Discriminant Analysis                           0.900260   0.916441   \n",
       "Linear Discriminant Analysis (10% of features)         0.865426   0.898544   \n",
       "Linear Discriminant Analysis (50% of features)         0.900409   0.926002   \n",
       "Linear Discriminant Analysis (PCA: 10 components)      0.890603   0.932323   \n",
       "Linear Discriminant Analysis (PCA: 3 components)       0.656592   0.651139   \n",
       "Linear Discriminant Analysis (scaled data)             0.900260   0.916441   \n",
       "Logistic Regression                                    0.895358   0.902952   \n",
       "Logistic Regression (10% of features)                  0.875451   0.906231   \n",
       "Logistic Regression (50% of features)                  0.905235   0.923947   \n",
       "Logistic Regression (PCA: 10 components)               0.880575   0.900637   \n",
       "Logistic Regression (PCA: 3 components)                0.602081   0.597002   \n",
       "Logistic Regression (scaled data)                      0.915263   0.925363   \n",
       "Random Forest Classifier                               0.790865   0.809829   \n",
       "Random Forest Classifier (10% of features)             0.855475   0.863882   \n",
       "Random Forest Classifier (50% of features)             0.875156   0.847446   \n",
       "Random Forest Classifier (PCA: 10 components)          0.806167   0.788286   \n",
       "Random Forest Classifier (PCA: 3 components)           0.637204   0.626672   \n",
       "Random Forest Classifier (scaled data)                 0.821015   0.766398   \n",
       "Support Vector Machine                                 0.587078   0.587078   \n",
       "Support Vector Machine (10% of features)               0.875451   0.906231   \n",
       "Support Vector Machine (50% of features)               0.905235   0.902284   \n",
       "Support Vector Machine (PCA: 10 components)            0.875598   0.914182   \n",
       "Support Vector Machine (PCA: 3 components)             0.592128   0.590108   \n",
       "Support Vector Machine (scaled data)                   0.757148   0.717507   \n",
       "\n",
       "                                                     Recall   F1Score  \\\n",
       "AdaBoost Classifier                                0.856197  0.845919   \n",
       "AdaBoost Classifier (10% of features)              0.872650  0.870417   \n",
       "AdaBoost Classifier (50% of features)              0.880769  0.858853   \n",
       "AdaBoost Classifier (PCA: 10 components)           0.880769  0.884277   \n",
       "AdaBoost Classifier (PCA: 3 components)            0.694444  0.673993   \n",
       "AdaBoost Classifier (scaled data)                  0.856197  0.845919   \n",
       "Decision Tree Classifier                           0.838889  0.823437   \n",
       "Decision Tree Classifier (10% of features)         0.889744  0.868386   \n",
       "Decision Tree Classifier (50% of features)         0.855769  0.819322   \n",
       "Decision Tree Classifier (PCA: 10 components)      0.796154  0.831298   \n",
       "Decision Tree Classifier (PCA: 3 components)       0.660470  0.655667   \n",
       "Decision Tree Classifier (scaled data)             0.864316  0.825941   \n",
       "Gaussian Naive Bayes                               0.821581  0.857743   \n",
       "Gaussian Naive Bayes (10% of features)             0.872650  0.892076   \n",
       "Gaussian Naive Bayes (50% of features)             0.838462  0.879274   \n",
       "Gaussian Naive Bayes (PCA: 10 components)          0.923291  0.915586   \n",
       "Gaussian Naive Bayes (PCA: 3 components)           0.804274  0.708947   \n",
       "Gaussian Naive Bayes (scaled data)                 0.821581  0.857743   \n",
       "Linear Discriminant Analysis                       0.914957  0.914979   \n",
       "Linear Discriminant Analysis (10% of features)     0.872650  0.884654   \n",
       "Linear Discriminant Analysis (50% of features)     0.906197  0.913924   \n",
       "Linear Discriminant Analysis (PCA: 10 components)  0.881197  0.905044   \n",
       "Linear Discriminant Analysis (PCA: 3 components)   0.897863  0.753714   \n",
       "Linear Discriminant Analysis (scaled data)         0.914957  0.914979   \n",
       "Logistic Regression                                0.923291  0.911746   \n",
       "Logistic Regression (10% of features)              0.881197  0.893122   \n",
       "Logistic Regression (50% of features)              0.914744  0.918516   \n",
       "Logistic Regression (PCA: 10 components)           0.898291  0.899071   \n",
       "Logistic Regression (PCA: 3 components)            0.991453  0.745238   \n",
       "Logistic Regression (scaled data)                  0.931624  0.927508   \n",
       "Random Forest Classifier                           0.889530  0.854762   \n",
       "Random Forest Classifier (10% of features)         0.906838  0.863169   \n",
       "Random Forest Classifier (50% of features)         0.898077  0.867908   \n",
       "Random Forest Classifier (PCA: 10 components)      0.864530  0.840541   \n",
       "Random Forest Classifier (PCA: 3 components)       0.728632  0.680917   \n",
       "Random Forest Classifier (scaled data)             0.923077  0.877445   \n",
       "Support Vector Machine                             1.000000  0.739816   \n",
       "Support Vector Machine (10% of features)           0.881197  0.893122   \n",
       "Support Vector Machine (50% of features)           0.940385  0.920732   \n",
       "Support Vector Machine (PCA: 10 components)        0.872863  0.892510   \n",
       "Support Vector Machine (PCA: 3 components)         1.000000  0.742197   \n",
       "Support Vector Machine (scaled data)               0.991453  0.830069   \n",
       "\n",
       "                                                        AUC  \n",
       "AdaBoost Classifier                                0.807507  \n",
       "AdaBoost Classifier (10% of features)              0.839543  \n",
       "AdaBoost Classifier (50% of features)              0.819573  \n",
       "AdaBoost Classifier (PCA: 10 components)           0.862122  \n",
       "AdaBoost Classifier (PCA: 3 components)            0.588183  \n",
       "AdaBoost Classifier (scaled data)                  0.807507  \n",
       "Decision Tree Classifier                           0.792674  \n",
       "Decision Tree Classifier (10% of features)         0.835524  \n",
       "Decision Tree Classifier (50% of features)         0.769794  \n",
       "Decision Tree Classifier (PCA: 10 components)      0.769421  \n",
       "Decision Tree Classifier (PCA: 3 components)       0.535375  \n",
       "Decision Tree Classifier (scaled data)             0.778549  \n",
       "Gaussian Naive Bayes                               0.844433  \n",
       "Gaussian Naive Bayes (10% of features)             0.875919  \n",
       "Gaussian Naive Bayes (50% of features)             0.871171  \n",
       "Gaussian Naive Bayes (PCA: 10 components)          0.895287  \n",
       "Gaussian Naive Bayes (PCA: 3 components)           0.584015  \n",
       "Gaussian Naive Bayes (scaled data)                 0.844433  \n",
       "Linear Discriminant Analysis                       0.896853  \n",
       "Linear Discriminant Analysis (10% of features)     0.863573  \n",
       "Linear Discriminant Analysis (50% of features)     0.898865  \n",
       "Linear Discriminant Analysis (PCA: 10 components)  0.892318  \n",
       "Linear Discriminant Analysis (PCA: 3 components)   0.606119  \n",
       "Linear Discriminant Analysis (scaled data)         0.896853  \n",
       "Logistic Regression                                0.889114  \n",
       "Logistic Regression (10% of features)              0.874020  \n",
       "Logistic Regression (50% of features)              0.903139  \n",
       "Logistic Regression (PCA: 10 components)           0.876614  \n",
       "Logistic Regression (PCA: 3 components)            0.519977  \n",
       "Logistic Regression (scaled data)                  0.911800  \n",
       "Random Forest Classifier                           0.830608  \n",
       "Random Forest Classifier (10% of features)         0.811822  \n",
       "Random Forest Classifier (50% of features)         0.841495  \n",
       "Random Forest Classifier (PCA: 10 components)      0.784481  \n",
       "Random Forest Classifier (PCA: 3 components)       0.584224  \n",
       "Random Forest Classifier (scaled data)             0.837556  \n",
       "Support Vector Machine                             0.500000  \n",
       "Support Vector Machine (10% of features)           0.874020  \n",
       "Support Vector Machine (50% of features)           0.897882  \n",
       "Support Vector Machine (PCA: 10 components)        0.876026  \n",
       "Support Vector Machine (PCA: 3 components)         0.506173  \n",
       "Support Vector Machine (scaled data)               0.708028  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "import datetime\n",
    "\n",
    "# has all the scikit-models in a dictionary with corresponding names\n",
    "models = {\"Logistic Regression\": LogisticRegression(), \"Linear Discriminant Analysis\": LinearDiscriminantAnalysis(),\n",
    "          \"Gaussian Naive Bayes\": GaussianNB(), \"Decision Tree Classifier\": DecisionTreeClassifier(),\n",
    "          \"Random Forest Classifier\": RandomForestClassifier(), \"AdaBoost Classifier\": AdaBoostClassifier(),\n",
    "          \"Support Vector Machine\": SVC()}\n",
    "\n",
    "# has all the input data transformations in a dictioanry with corresponding names\n",
    "data = {\"\": X_train, \"(scaled data)\": X_train_scaled, \"(PCA: 10 components)\": X_train_pca10, \"(PCA: 3 components)\": X_train_pca3,\n",
    "        \"(50% of features)\": X_train_reduce50, \"(10% of features)\": X_train_reduce10}\n",
    "\n",
    "res = {}\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        for scale, X in data.items():\n",
    "            n = name + \" \" + scale\n",
    "            clf = model\n",
    "            \n",
    "            # a function which converts a metric function to a callable function\n",
    "            default_recall = make_scorer(recall_score, pos_label=0, average=\"binary\")\n",
    "            default_precision = make_scorer(precision_score, pos_label=0, average=\"binary\")\n",
    "            default_fscore = make_scorer(f1_score, pos_label=0, average=\"binary\")\n",
    "            default_auc = make_scorer(roc_auc_score)\n",
    "            \n",
    "            # taking 3 fold classification average results of the below mentioned metrics over X_train itself\n",
    "            acc = cross_val_score(clf, X, y_train).mean()\n",
    "            rec = cross_val_score(clf, X, y_train, scoring=default_recall).mean()\n",
    "            prec = cross_val_score(clf, X, y_train, scoring=default_precision).mean()\n",
    "            f1 = cross_val_score(clf, X, y_train, scoring=default_fscore).mean()\n",
    "            auc = cross_val_score(clf, X, y_train, scoring=default_auc).mean()\n",
    "\n",
    "\n",
    "            res[n] = {\"MeanAccuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1Score\": f1, \n",
    "                      \"AUC\": auc}\n",
    "\n",
    "results = pd.DataFrame.from_dict(res, orient=\"index\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Logistic Regression (scaled data)', 0.9275081643502695)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# has the hightest f1-score\n",
    "results[\"F1Score\"].idxmax(), results[\"F1Score\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Logistic Regression (scaled data)', 0.9117996201329536)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# has the highest area under the curve, which seems to be a reasonable metric since data is slightly unbalanced\n",
    "results[\"AUC\"].idxmax(), results[\"AUC\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can see that linear models like Logistic Regression and Linear Discriminant Analysis have advantage over ensemble and kernel based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "clf = xgb.XGBClassifier(learning_rate=0.1, n_estimators=140, max_depth=5,\n",
    "                        min_child_weight=3, gamma=0.2, subsample=0.6, colsample_bytree=1.0,\n",
    "                        objective='binary:logistic', nthread=4, scale_pos_weight=1, seed=27)\n",
    "\n",
    "xgb_param = clf.get_xgb_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MeanAccuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1Score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGBoost Classifier</th>\n",
       "      <td>0.875451</td>\n",
       "      <td>0.889182</td>\n",
       "      <td>0.880983</td>\n",
       "      <td>0.884444</td>\n",
       "      <td>0.861788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost Classifier (10% of features)</th>\n",
       "      <td>0.875451</td>\n",
       "      <td>0.884253</td>\n",
       "      <td>0.898077</td>\n",
       "      <td>0.890890</td>\n",
       "      <td>0.864162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost Classifier (50% of features)</th>\n",
       "      <td>0.875451</td>\n",
       "      <td>0.907172</td>\n",
       "      <td>0.889530</td>\n",
       "      <td>0.897297</td>\n",
       "      <td>0.877966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost Classifier (PCA: 10 components)</th>\n",
       "      <td>0.875451</td>\n",
       "      <td>0.884097</td>\n",
       "      <td>0.855342</td>\n",
       "      <td>0.864182</td>\n",
       "      <td>0.843235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost Classifier (PCA: 3 components)</th>\n",
       "      <td>0.875451</td>\n",
       "      <td>0.611934</td>\n",
       "      <td>0.711325</td>\n",
       "      <td>0.657081</td>\n",
       "      <td>0.536438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost Classifier (scaled data)</th>\n",
       "      <td>0.875451</td>\n",
       "      <td>0.889182</td>\n",
       "      <td>0.880983</td>\n",
       "      <td>0.884444</td>\n",
       "      <td>0.861788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         MeanAccuracy  Precision    Recall  \\\n",
       "XGBoost Classifier                           0.875451   0.889182  0.880983   \n",
       "XGBoost Classifier (10% of features)         0.875451   0.884253  0.898077   \n",
       "XGBoost Classifier (50% of features)         0.875451   0.907172  0.889530   \n",
       "XGBoost Classifier (PCA: 10 components)      0.875451   0.884097  0.855342   \n",
       "XGBoost Classifier (PCA: 3 components)       0.875451   0.611934  0.711325   \n",
       "XGBoost Classifier (scaled data)             0.875451   0.889182  0.880983   \n",
       "\n",
       "                                          F1Score       AUC  \n",
       "XGBoost Classifier                       0.884444  0.861788  \n",
       "XGBoost Classifier (10% of features)     0.890890  0.864162  \n",
       "XGBoost Classifier (50% of features)     0.897297  0.877966  \n",
       "XGBoost Classifier (PCA: 10 components)  0.864182  0.843235  \n",
       "XGBoost Classifier (PCA: 3 components)   0.657081  0.536438  \n",
       "XGBoost Classifier (scaled data)         0.884444  0.861788  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = {\"XGBoost Classifier\": xgb.XGBClassifier()}\n",
    "\n",
    "data = {\"\": X_train.values, \"(scaled data)\": X_train_scaled, \"(PCA: 10 components)\": X_train_pca10, \"(PCA: 3 components)\": X_train_pca3,\n",
    "        \"(50% of features)\": X_train_reduce50, \"(10% of features)\": X_train_reduce10}\n",
    "\n",
    "res = {}\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    for name, model in models.items():\n",
    "        for scale, X in data.items():\n",
    "                n = name + \" \" + scale\n",
    "                clf = model\n",
    "\n",
    "                xgtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n",
    "                cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=clf.get_params()['n_estimators'], nfold=3,\n",
    "                                          early_stopping_rounds=50)\n",
    "                clf.set_params(n_estimators=cvresult.shape[0])\n",
    "                clf.fit(X_train.values, y_train, eval_metric='auc')            \n",
    "\n",
    "                default_recall = make_scorer(recall_score, pos_label=0, average=\"binary\")\n",
    "                default_precision = make_scorer(precision_score, pos_label=0, average=\"binary\")\n",
    "                default_fscore = make_scorer(f1_score, pos_label=0, average=\"binary\")\n",
    "                default_auc = make_scorer(roc_auc_score)\n",
    "                rec = cross_val_score(clf, X, y_train.values, scoring=default_recall).mean()\n",
    "                prec = cross_val_score(clf, X, y_train.values, scoring=default_precision).mean()\n",
    "                f1 = cross_val_score(clf, X, y_train.values, scoring=default_fscore).mean()\n",
    "                auc = cross_val_score(clf, X, y_train.values, scoring=default_auc).mean()\n",
    "                res[n] = {\"MeanAccuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1Score\": f1, \"AUC\": auc,}\n",
    "\n",
    "\n",
    "results_xgb = pd.DataFrame.from_dict(res, orient=\"index\")\n",
    "results_xgb       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "\n",
    "params = {\n",
    "    'objective' :'binary',\n",
    "    'learning_rate' : 0.02,\n",
    "    'num_leaves' : 76,\n",
    "    'feature_fraction': 0.64, \n",
    "    'bagging_fraction': 0.8, \n",
    "    'bagging_freq':1,\n",
    "    'boosting_type' : 'gbdt',\n",
    "    'metric': 'auc',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[85]\tvalid_0's auc: 0.923077\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[107]\tvalid_0's auc: 0.925926\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[359]\tvalid_0's auc: 0.959707\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[85]\tvalid_0's auc: 0.923077\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[107]\tvalid_0's auc: 0.925926\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[359]\tvalid_0's auc: 0.959707\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[425]\tvalid_0's auc: 0.96337\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[554]\tvalid_0's auc: 0.919444\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[352]\tvalid_0's auc: 0.901099\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's auc: 0.569139\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's auc: 0.682407\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[28]\tvalid_0's auc: 0.585165\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[145]\tvalid_0's auc: 0.925824\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[381]\tvalid_0's auc: 0.943519\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[202]\tvalid_0's auc: 0.96337\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's auc: 0.953755\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[71]\tvalid_0's auc: 0.923148\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[379]\tvalid_0's auc: 0.960623\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MeanAccuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1Score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LightGBM Classifier</th>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.827151</td>\n",
       "      <td>0.794533</td>\n",
       "      <td>0.808127</td>\n",
       "      <td>0.837971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM Classifier (10% of features)</th>\n",
       "      <td>0.771144</td>\n",
       "      <td>0.557778</td>\n",
       "      <td>0.556878</td>\n",
       "      <td>0.556587</td>\n",
       "      <td>0.740405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM Classifier (50% of features)</th>\n",
       "      <td>0.875622</td>\n",
       "      <td>0.825530</td>\n",
       "      <td>0.891975</td>\n",
       "      <td>0.855881</td>\n",
       "      <td>0.878359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM Classifier (PCA: 10 components)</th>\n",
       "      <td>0.825871</td>\n",
       "      <td>0.783626</td>\n",
       "      <td>0.832892</td>\n",
       "      <td>0.798718</td>\n",
       "      <td>0.827985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM Classifier (PCA: 3 components)</th>\n",
       "      <td>0.577114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.491453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM Classifier (scaled data)</th>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.827151</td>\n",
       "      <td>0.794533</td>\n",
       "      <td>0.808127</td>\n",
       "      <td>0.837971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          MeanAccuracy  Precision    Recall  \\\n",
       "LightGBM Classifier                           0.845771   0.827151  0.794533   \n",
       "LightGBM Classifier (10% of features)         0.771144   0.557778  0.556878   \n",
       "LightGBM Classifier (50% of features)         0.875622   0.825530  0.891975   \n",
       "LightGBM Classifier (PCA: 10 components)      0.825871   0.783626  0.832892   \n",
       "LightGBM Classifier (PCA: 3 components)       0.577114   0.000000  0.000000   \n",
       "LightGBM Classifier (scaled data)             0.845771   0.827151  0.794533   \n",
       "\n",
       "                                           F1Score       AUC  \n",
       "LightGBM Classifier                       0.808127  0.837971  \n",
       "LightGBM Classifier (10% of features)     0.556587  0.740405  \n",
       "LightGBM Classifier (50% of features)     0.855881  0.878359  \n",
       "LightGBM Classifier (PCA: 10 components)  0.798718  0.827985  \n",
       "LightGBM Classifier (PCA: 3 components)   0.000000  0.491453  \n",
       "LightGBM Classifier (scaled data)         0.808127  0.837971  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "models = {\"LightGBM Classifier\": lgbm}\n",
    "\n",
    "data = {\"\": X_train.values, \"(scaled data)\": X_train_scaled, \"(PCA: 10 components)\": X_train_pca10, \"(PCA: 3 components)\": X_train_pca3,\n",
    "        \"(50% of features)\": X_train_reduce50, \"(10% of features)\": X_train_reduce10}\n",
    "\n",
    "res = {}\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    for name, model in models.items():\n",
    "        for scale, X in data.items():\n",
    "                n = name + \" \" + scale\n",
    "                clf = model\n",
    "\n",
    "                kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "                rec_scores, prec_scores, f1_scores, auc_scores, acc_scores = [], [], [], [], []\n",
    "\n",
    "                for train_index, valid_index in kfold.split(X):\n",
    "\n",
    "                    X_t_fold, X_v_fold = X[train_index], X[valid_index]\n",
    "                    y_t_fold, y_v_fold = y_train.values[train_index], y_train.values[valid_index]\n",
    "\n",
    "                    d_t_fold = lgbm.Dataset(X_t_fold, y_t_fold)\n",
    "                    d_v_fold = lgbm.Dataset(X_v_fold, y_v_fold)\n",
    "\n",
    "                    clf = lgbm.train(params, d_t_fold, 5000, valid_sets=[d_v_fold], verbose_eval=5000, early_stopping_rounds=100)\n",
    "                    y_pred_fold = clf.predict(X_v_fold)\n",
    "\n",
    "                    y_pred_fold[y_pred_fold < 0.5] = 0\n",
    "                    y_pred_fold[y_pred_fold >= 0.5] = 1\n",
    "\n",
    "                    rec = recall_score(y_v_fold, y_pred_fold)\n",
    "                    prec = precision_score(y_v_fold, y_pred_fold)\n",
    "                    f1 = f1_score(y_v_fold, y_pred_fold)\n",
    "                    auc = roc_auc_score(y_v_fold, y_pred_fold)\n",
    "                    acc = accuracy_score(y_v_fold, y_pred_fold)\n",
    "\n",
    "                    rec_scores.append(rec)\n",
    "                    prec_scores.append(prec)\n",
    "                    f1_scores.append(f1)\n",
    "                    auc_scores.append(auc)\n",
    "                    acc_scores.append(acc)\n",
    "\n",
    "\n",
    "                res[n] = {\"MeanAccuracy\": np.mean(acc_scores), \"Precision\": np.mean(prec_scores), \n",
    "                          \"Recall\": np.mean(rec_scores), \"F1Score\": np.mean(f1_scores), \n",
    "                          \"AUC\": np.mean(auc_scores)}\n",
    "\n",
    "\n",
    "results_lgbm = pd.DataFrame.from_dict(res, orient=\"index\")\n",
    "results_lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best model so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model which had good results during cross-validation generalizes well on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9642857142857143"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_valid)\n",
    "f1_score(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proceeding with test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since y_test is given, we'd like to try and see how well it performs\n",
    "X_test = df_test.drop(['Winner (team 1=1, team 2=0)'], axis = 1)\n",
    "y_test = df_test['Winner (team 1=1, team 2=0)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8749999999999999"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8947368421052632"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9740896358543416"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba = clf.predict_proba(X_test)[:,1]\n",
    "roc_auc_score(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.95      0.91        42\n",
      "          1       0.93      0.82      0.87        34\n",
      "\n",
      "avg / total       0.90      0.89      0.89        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction for each match results can be found in `y_pred` and probabilites of them winning is in `y_pred_proba`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model achieves an `AUC` of ~0.974, accurately classifies upto ~89.47% and has a `f1 score` of 0.875."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things I tried that I'd like to mention\n",
    "1. Firstly, trained the model without DateOfGame and TimeOfGame, the model actually has better accuracy.\n",
    "2. Secondly, trained the model with Game ID, the model again has better accuracy.\n",
    "3. Initially used a dictionary to map the team names, and dropped HSR related `CityOfGame`, `Team 1`, `Team 2`from `df_test`, has slightly better accuracy (~91.38%) but at the cost of ~20(out 76) HSR related observations.\n",
    "\n",
    "but rejected all from a perspective of causal reasoning and federated learning.\n",
    "\n",
    "### Why?\n",
    "\n",
    "* DateOfGame and TimeOfGame have a physical significance and anyone can argue a team might be stronger or weaker than how it was. So it should be a cause for making a prediction. \n",
    "* Game ID is just a number which keeps on increasing, which can make the learning algorithm think if it's high chances of winning should be low / high, I personally think that's why my DecisionTree failed during that time.\n",
    "* As mentioned above earlier, we can't assume there won't be new teams or new cities for stadiums, therefore we induce new classes to handle new teams and cities under the roof of `other`, which can help us make predictions based on other features, though it is not advised during a sensitive predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
